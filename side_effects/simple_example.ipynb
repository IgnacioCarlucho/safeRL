{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the simple experiment as described in the paper \"Penalizing side effects using stepwise relative reachability\" - Krakovna et al. (https://arxiv.org/pdf/1806.01186.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment\n",
    "\n",
    "T = {}\n",
    "T['s1'] = {'b1':'s2','b2':'s3','noop':'s1'}\n",
    "T['s2'] = {'b1':'s2','b2':'s4','noop':'s2'}\n",
    "T['s3'] = {'b1':'s4','b2':'s3','noop':'s3'}\n",
    "T['s4'] = {'b1':'s4','b2':'s4','noop':'s4'}\n",
    "\n",
    "E ={'0':'s1','1':'s2','2':'s3','3':'s4'}\n",
    "E_a = {'0':'b1','1':'b2','2':'noop'}\n",
    "E_rev = {'s1':'0','s2':'1','s3':'2','s4':'3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://github.com/hari-sikchi/safeRL/blob/safe_recovery/side_effects/env.png\" ,width = 500,height =500>\n",
    "This is the environment that we have defined in the previous block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple method implement tabular q learning for the environment shown above\n",
    "def q_learning():\n",
    "    n_states = 4\n",
    "    n_actions = 3\n",
    "    q_table = np.zeros((n_states,n_actions))\n",
    "    alpha = 0.8\n",
    "    n_episodes = 50\n",
    "    gamma = 0.9\n",
    "    for i in range(n_episodes):\n",
    "        transitions = sample_transitions(q_table)\n",
    "        for transition in transitions:\n",
    "            q_table[int(E_rev[transition[0]])][transition[1]] = alpha*q_table[int(E_rev[transition[0]]),transition[1]] + (1-alpha)*(transition[2]+gamma*np.max(q_table[int(E_rev[transition[3]]),:]))\n",
    "\n",
    "\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method samples transitions from the environment shown above\n",
    "def sample_transitions(q_table,horizon = 4,epsilon = 0.8):\n",
    "    \n",
    "    transitions = []\n",
    "    obs = 's1'\n",
    "\n",
    "    for timestep in range(horizon):\n",
    "        if np.random.random()<epsilon:\n",
    "            action = np.argmax(q_table[int(E_rev[obs]),:])\n",
    "            action_str = E_a[str(action)]\n",
    "        else:\n",
    "            action = np.random.randint(0,3)\n",
    "            action_str = E_a[str(action)]\n",
    "\n",
    "        next_obs = T[obs][action_str]\n",
    "        reward = - compute_relative_reachablity(next_obs,obs)\n",
    "        transitions.append([obs,action,reward,next_obs])\n",
    "        obs = next_obs\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative reachability measure as explained in the paper: Penalizing side effects using stepwise relative reachability by Krakovna et al.\n",
    "def compute_relative_reachablity(state1,state2):\n",
    "    global T,E\n",
    "    gamma = 0.99\n",
    "    reachability = np.full((4,4),-np.inf)\n",
    "    for i in range(4):\n",
    "        reachability[i,i]=1\n",
    "\n",
    "    for iter_ in range(10):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                temp = 0\n",
    "                if i!=j:\n",
    "                    for a in range(3):\n",
    "                            temp = max(reachability[int(E_rev[T[str(E[str(i)])][E_a[str(a)]]]),j],temp)  \n",
    "                    \n",
    "                    reachability[i,j]= gamma * temp\n",
    "\n",
    "\n",
    "    rel_reach = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(4):\n",
    "        rel_reach+= max(reachability[int(E_rev[state1]),i]-reachability[int(E_rev[state2]),i],0)\n",
    "        # print(rel_reach)\n",
    "        count+=1\n",
    "\n",
    "    return (rel_reach/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00393167 -0.00492911  0.        ]\n",
      " [ 0.         -0.00122     0.        ]\n",
      " [-0.0009      0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #print(compute_relative_reachablity('s3','s2'))\n",
    "    q_learning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned Q-values indicate that the best action to take when starting from state 1 is to do nothing. i.e \"noop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
